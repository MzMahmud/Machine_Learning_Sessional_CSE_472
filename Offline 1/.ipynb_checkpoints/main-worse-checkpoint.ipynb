{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    \"\"\"\n",
    "        entropy(data)\n",
    "        = -sum val in class P(data = val) * log2(P(data = val))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    total_count = len(data)\n",
    "    entropy_val = 0\n",
    "    \n",
    "    _,counts    = np.unique(data,return_counts=True)\n",
    "    for count in counts:\n",
    "        Prob = count/total_count\n",
    "        entropy_val += (Prob * np.log2(Prob))\n",
    "        \n",
    "    return -entropy_val\n",
    "\n",
    "\n",
    "def information_gain(attribute,data,target = \"class\"):\n",
    "    \n",
    "    \"\"\"\n",
    "        Information_Gain(attribute,data)\n",
    "        = entropy(data) - entropy(feature,data)\n",
    "        \n",
    "        entropy(feature,data) \n",
    "        = weighted sum of entropy for every value of fuature\n",
    "    \"\"\"\n",
    "    \n",
    "    total_entropy = entropy(data[target])\n",
    "    \n",
    "    total_count      = len(data)\n",
    "    weighted_entropy = 0\n",
    "    \n",
    "    values,counts = np.unique(data[attribute],return_counts = True)\n",
    "    for value,count in zip(values,counts):\n",
    "        # data for a specific value\n",
    "        data_for_value = data.loc[data[attribute] == value][target]\n",
    "        weighted_entropy += (count/total_count * entropy(data_for_value))\n",
    "        \n",
    "    return total_entropy - weighted_entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID3 Algo \n",
    "\n",
    "def plurality_value(examples,target):\n",
    "    \"\"\"\n",
    "        input: \n",
    "                examples - the example\n",
    "                target   - the name of target\n",
    "        output:\n",
    "                the most occuring target in examples\n",
    "    \"\"\"\n",
    "    values,counts = np.unique(examples[target],return_counts=True)\n",
    "    max_index     = np.argmax(counts)\n",
    "    return values[max_index]\n",
    "\n",
    "\n",
    "def decision_tree(examples,attributes,parent_examples,depth):\n",
    "    # target name is at the last column name of example\n",
    "    target = examples.columns[-1]\n",
    "    \n",
    "    # cases\n",
    "    \n",
    "    # stopping criteria\n",
    "    #-----------------------------------------------------\n",
    "    # 1. examples is empty\n",
    "    if len(examples) == 0:\n",
    "        return plurality_value(parent_examples,target)\n",
    "    \n",
    "    # 2. all the example are same\n",
    "    unique_values = np.unique(examples[target])\n",
    "    if len(unique_values) <= 1:\n",
    "        return unique_values[0]\n",
    "    \n",
    "    # 3. attributes is empty or depth limit reached\n",
    "    if len(attributes) == 0 or depth == 0:\n",
    "        return plurality_value(examples,target)\n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    # 4. grow the tree\n",
    "    \n",
    "    #information gain for each attributes\n",
    "    importance = [ information_gain(attribute,examples,target) for attribute in attributes ]\n",
    "    \n",
    "    # best information gain\n",
    "    best_attribute = attributes[np.argmax(importance)]\n",
    "    \n",
    "    # tree with best feature as root\n",
    "    tree = {best_attribute:{}}\n",
    "    \n",
    "    # attributes = attributes - best_attribute\n",
    "    #attributes.remove(best_attribute)\n",
    "    attributes = [ a for a in attributes if a != best_attribute]\n",
    "    print(attributes)\n",
    "    \n",
    "    # for each value in best_attribute\n",
    "    for value in np.unique(examples[best_attribute]):\n",
    "        \n",
    "        # split the examples on attrivute values\n",
    "        split_examples = examples.loc[examples[best_attribute] == value]\n",
    "        \n",
    "        # recursively build the subtree\n",
    "        subtree = decision_tree(split_examples,attributes,examples,depth - 1)\n",
    "        \n",
    "        # ddd the sub tree under root \n",
    "        tree[best_attribute][value] = subtree\n",
    "    \n",
    "    # if no value match then default = plurality in the node\n",
    "    tree[best_attribute][\"__default__\"] = plurality_value(examples,target)\n",
    "\n",
    "    return tree\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict a query\n",
    "\n",
    "def get_tree_node(query,tree):\n",
    "    \"\"\"\n",
    "        return - matching feature node of query in the tree\n",
    "    \"\"\"\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                # if the value does not exist in tree branch\n",
    "                # that means new data\n",
    "                # return the default branch\n",
    "                return tree[key][\"__default__\"]\n",
    "            \n",
    "            # written again for python scope -_-\n",
    "            result = tree[key][query[key]]\n",
    "            return result \n",
    "\n",
    "        \n",
    "def is_leaf(node):\n",
    "    \"\"\"\n",
    "        in this design if node is a dictionary its not a leaf\n",
    "        if its just a value then its the class type\n",
    "    \"\"\"\n",
    "    return not isinstance(node,dict)\n",
    "\n",
    "\n",
    "def dt_predict(query,tree):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        query - in for if a dictionary {feature1: value1,feature2: value2,....}\n",
    "        tree  - decsion tree in the form of dictionary of dictionaries\n",
    "        \n",
    "    output:\n",
    "        a target class\n",
    "    \"\"\"\n",
    "    \n",
    "    # get the corresponding tree node\n",
    "    node = get_tree_node(query,tree)\n",
    "    \n",
    "    # if the node is leaf just return the result\n",
    "    if is_leaf(node):\n",
    "        return node\n",
    "    \n",
    "    # recurse on the node \n",
    "    return dt_predict(query,node)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_get_pred_true(test_examples,tree):\n",
    "    # removes last column that contains class\n",
    "    # converts each row as a dictionary of columns\n",
    "    queries = test_examples.iloc[:,:-1].to_dict(orient='records')\n",
    "    \n",
    "    y_pred = [] \n",
    "    y_true = [] \n",
    "    for i in range(len(test_examples)):\n",
    "        prediction = dt_predict(queries[i],tree)\n",
    "        y_pred.append(prediction)\n",
    "        y_true.append(test_examples.iloc[i,-1])\n",
    "    \n",
    "    return y_pred,y_true\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaboost\n",
    "\n",
    "def resample(examples,w):\n",
    "    \"\"\"\n",
    "        n = |examples|\n",
    "        resamples n value with replacement from examples\n",
    "        with probability w\n",
    "    \"\"\"\n",
    "    n = len(examples.index)\n",
    "    # chooses randon indices based on a probability array\n",
    "    indices = np.random.choice(range(n),n,p = w)\n",
    "    return examples.iloc[indices]\n",
    "\n",
    "        \n",
    "def adaboost(examples,algo,total_hypotheses):\n",
    "    n = len(examples.index)\n",
    "    w = np.full(n, 1/n,dtype='float64')\n",
    "    \n",
    "    # initilize a k size array \n",
    "    h = [0 for _ in range(total_hypotheses)]\n",
    "    z = [0 for _ in range(total_hypotheses)]\n",
    "    for k in range(total_hypotheses):\n",
    "        \n",
    "        #resamples data based on weight\n",
    "        data = resample(examples,w)\n",
    "        \n",
    "        #attributes are values of column apart from the last one\n",
    "        attributes = list(examples.columns[:-1])\n",
    "        h[k] = algo(data,attributes,data,1) # train stump->decsion tree with depth 1\n",
    "        \n",
    "        error = .0\n",
    "        for j in range(n):\n",
    "            query        = examples.iloc[j,:-1].to_dict()\n",
    "            prediction   = dt_predict(query,h[k])\n",
    "            actual_class = examples.iloc[j,-1]\n",
    "            if prediction != actual_class:\n",
    "                error += w[j]\n",
    "        \n",
    "        if error > .5:\n",
    "            continue\n",
    "            \n",
    "        for j in range(n):\n",
    "            query        = examples.iloc[j,:-1].to_dict()\n",
    "            prediction   = dt_predict(query,h[k])\n",
    "            actual_class = examples.iloc[j,-1]\n",
    "            if prediction == actual_class:\n",
    "                w[j] *= (error/(1 - error))\n",
    "        \n",
    "        # normalize w\n",
    "        w = [ val/sum(w) for val in w ]\n",
    "        \n",
    "        if error == 0:\n",
    "            z[k] = float('inf')\n",
    "        else:\n",
    "            z[k] = np.log((1 - error)/error)\n",
    "    \n",
    "    return (h,z)\n",
    "\n",
    "def adaboost_predict(query,h,z):\n",
    "    # stores the vote for each prediction as {p : v} dictionary\n",
    "    pred_vote = dict()\n",
    "    for i in range(len(h)):\n",
    "        pred = dt_predict(query,h[i])\n",
    "        if pred in pred_vote:\n",
    "            pred_vote[pred] += z[i]\n",
    "        else:\n",
    "            pred_vote[pred]  = z[i]\n",
    "        \n",
    "    # return the key with maximum count\n",
    "    # ie, prediction with maximum vote\n",
    "    return max(pred_vote.keys(), key=(lambda k: pred_vote[k]))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost_get_pred_true(test_examples,h,z):\n",
    "    # removes last column that contains class\n",
    "    # converts each row as a dictionary of columns\n",
    "    queries = test_examples.iloc[:,:-1].to_dict(orient='records')\n",
    "    \n",
    "    y_pred = [] \n",
    "    y_true = [] \n",
    "    for i in range(len(test_examples)):\n",
    "        prediction = adaboost_predict(queries[i],h,z)\n",
    "        y_pred.append(prediction)\n",
    "        y_true.append(test_examples.iloc[i,-1])\n",
    "    \n",
    "    return y_pred,y_true\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Import the set and define the feature as well as the target sets / columns#\n",
    "examples = pd.read_csv('data/zoo.csv')#, names=attributes)\n",
    "attributes = list(examples.columns)[1:-1]\n",
    "#print(attributes)\n",
    "\n",
    "#Import all columns omitting the fist which consists the names of the animals\n",
    "#We drop the animal names since this is not a good feature to split the  on\n",
    "examples = examples.drop(columns='animal_name')\n",
    "\n",
    "train_examples,test_examples = train_test_split(examples,test_size=0.2, random_state=1)\n",
    "depth      = len(attributes)\n",
    "\n",
    "tree = decision_tree(examples,attributes,train_examples,depth)\n",
    "pprint(tree)\n",
    "test(test_examples,tree)\n",
    "\n",
    "#h,z = adaboost(train_examples,decision_tree,20)\n",
    "\n",
    "#print(h,z)\n",
    "#adaboost_test(test_examples,h,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance measure\n",
    "def confusion_matrix_2(y_pred,y_true,pos,neg):\n",
    "    \"\"\"\n",
    "    inout:\n",
    "        y_pred - prediction value\n",
    "        y_true - true value\n",
    "        pos    - positive attribute\n",
    "        neg    - negative attribute\n",
    "        \n",
    "    output:\n",
    "        a diftionary with TruePositive,FalsePositive,FalseNegative,TrueNegative\n",
    "    \"\"\"\n",
    "    confusion_matrix = {'TP': 0 ,'FP' : 0,'FN': 0 ,'TN' : 0}\n",
    "    \n",
    "    for pred,true in zip(y_pred,y_true):\n",
    "        if pred == pos and true == pos:\n",
    "            confusion_matrix['TP'] += 1\n",
    "        \n",
    "        elif pred == pos and true == neg:\n",
    "            confusion_matrix['FP'] += 1\n",
    "            \n",
    "        elif pred == neg and true == pos:\n",
    "            confusion_matrix['FN'] += 1\n",
    "            \n",
    "        elif pred == neg and true == neg:\n",
    "            confusion_matrix['TN'] += 1\n",
    "            \n",
    "    return confusion_matrix\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stat\n",
    "def get_stat(confusion_matrix):\n",
    "    TP = confusion_matrix['TP']\n",
    "    TN = confusion_matrix['TN']\n",
    "    FP = confusion_matrix['FP']\n",
    "    FN = confusion_matrix['FN']\n",
    "    \n",
    "    accuracy = (TP + TN)/(TP + TN + FP + FN)*100\n",
    "    print(f'Accuracy\\t\\t{accuracy}%')\n",
    "    \n",
    "    sensitivity = TP/(TP + FN)*100\n",
    "    print(f'Sensitivity\\t\\t{sensitivity}%')\n",
    "    \n",
    "    specificity = TN/(TN + FP)*100\n",
    "    print(f'Specificity\\t\\t{specificity}%')\n",
    "    \n",
    "    precision = TP/(TP + FP)*100\n",
    "    print(f'precision\\t\\t{precision}%')\n",
    "    \n",
    "    false_discovery_rate = FP/(FP + TP)*100\n",
    "    print(f'False Discovery Rate\\t{false_discovery_rate}%')\n",
    "    \n",
    "    f1_score = (2*TP)/(2*TP + FP + FN)*100\n",
    "    print(f'F1 score\\t\\t{f1_score}%')\n",
    "    \n",
    "    \n",
    "def show_dt_stat(examples,train_examples,test_examples_list,pos,neg):\n",
    "    # Parse parameter from example\n",
    "    attributes = list(examples.columns)[:-1]\n",
    "    depth      = len(attributes)\n",
    "    \n",
    "    # Train decision tree \n",
    "    print('****Start Training****')\n",
    "    tree = decision_tree(examples,attributes,train_examples,depth)\n",
    "    print('****Training Done****')\n",
    "    #pprint(tree)\n",
    "    \n",
    "    # for each specified test examples show stat\n",
    "    for test_examples in test_examples_list:\n",
    "        # Get prediction\n",
    "        print('\\n\\n*******Testing*******')\n",
    "        y_pred,y_true = dt_get_pred_true(test_examples,tree)\n",
    "        \n",
    "        # get confusion_matrix \n",
    "        print('*****Testing Done*****\\n')\n",
    "        print('*********Stat*********')\n",
    "        get_stat(confusion_matrix_2(y_pred,y_true,pos,neg))\n",
    "    \n",
    "\n",
    "def show_ab_stat(train_examples,algo,test_examples_list,n_round_list,pos,neg):\n",
    "    # for each specified round generate stat\n",
    "    for n_round in n_round_list:\n",
    "        # Train adaboost\n",
    "        print('******Training*******')\n",
    "        h,z = adaboost(train_examples,algo,n_round)\n",
    "        print('****Training Done****')\n",
    "        #print(h,z)\n",
    "        \n",
    "        # for each specified test examples show stat\n",
    "        for test_examples in test_examples_list:\n",
    "            # Get prediction\n",
    "            print('\\n\\n*******Testing*******')\n",
    "            y_pred,y_true = adaboost_get_pred_true(test_examples,h,z)\n",
    "\n",
    "            # get confusion_matrix \n",
    "            print('*****Testing Done*****\\n')\n",
    "            print('*********Stat*********')\n",
    "            print(f'----K = {n_round}----')\n",
    "            get_stat(confusion_matrix_2(y_pred,y_true,pos,neg))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****Start Training****\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'OnlineBackup', 'DeviceProtection', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'OnlineBackup', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'OnlineBackup', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'OnlineBackup', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Dependents', 'tenure', 'PhoneService', 'OnlineBackup', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges']\n",
      "['gender', 'SeniorCitizen', 'Dependents', 'tenure', 'PhoneService', 'OnlineBackup', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges']\n",
      "['gender', 'Dependents', 'tenure', 'PhoneService', 'OnlineBackup', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges']\n",
      "['gender', 'Dependents', 'tenure', 'PhoneService', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges']\n",
      "['Dependents', 'tenure', 'PhoneService', 'StreamingMovies', 'PaperlessBilling', 'MonthlyCharges']\n",
      "['Dependents', 'tenure', 'PhoneService', 'StreamingMovies', 'MonthlyCharges']\n",
      "['tenure', 'PhoneService', 'StreamingMovies', 'MonthlyCharges']\n",
      "['PhoneService', 'StreamingMovies', 'MonthlyCharges']\n",
      "['StreamingMovies', 'MonthlyCharges']\n",
      "['MonthlyCharges']\n",
      "[]\n",
      "****Training Done****\n",
      "\n",
      "\n",
      "*******Testing*******\n",
      "*****Testing Done*****\n",
      "\n",
      "*********Stat*********\n",
      "Accuracy\t\t77.17333333333333%\n",
      "Sensitivity\t\t68.79574184963406%\n",
      "Specificity\t\t80.22804463852499%\n",
      "precision\t\t55.92212006489995%\n",
      "False Discovery Rate\t44.07787993510006%\n",
      "F1 score\t\t61.6945107398568%\n",
      "\n",
      "\n",
      "*******Testing*******\n",
      "*****Testing Done*****\n",
      "\n",
      "*********Stat*********\n",
      "Accuracy\t\t76.19047619047619%\n",
      "Sensitivity\t\t69.12568306010928%\n",
      "Specificity\t\t78.6743515850144%\n",
      "precision\t\t53.26315789473684%\n",
      "False Discovery Rate\t46.73684210526316%\n",
      "F1 score\t\t60.166468489892985%\n"
     ]
    }
   ],
   "source": [
    "# Load Example\n",
    "examples = pd.read_csv('data/telco.csv')\n",
    "\n",
    "# Train-Test Split \n",
    "train_examples,test_examples = train_test_split(examples,test_size=0.2, random_state=1)\n",
    "\n",
    "show_dt_stat(examples,train_examples,[train_examples,test_examples],'Yes','No')\n",
    "\n",
    "#show_ab_stat(train_examples,decision_tree,[train_examples,test_examples],[20],'Yes','No')\n",
    "#show_ab_stat(train_examples,decision_tree,[test_examples],[5,10,15],'Yes','No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
